{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Recommendation system using K-arm bandits","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\n\nclass KArmBandit:\n    def __init__(self, k, epsilon=0.1):\n        self.k = k  # Nuo.of arms\n        self.epsilon = epsilon  # rate of exploration\n        self.counts = np.zeros(k)\n        self.values = np.zeros(k) # reward estimated\n        \n    def select_arm(self):\n        if random.random() > self.epsilon:\n            return np.argmax(self.values) # exploitation\n        else:\n            return random.randint(0, self.k - 1) # exploration\n        \n    def update(self, chosen_arm, reward):\n        self.counts[chosen_arm] += 1\n        n = self.counts[chosen_arm]\n        value = self.values[chosen_arm]\n        # New estimated value\n        new_value = value + (reward - value) / n\n        self.values[chosen_arm] = new_value\n\n    def recommend(self):\n        chosen_arm = self.select_arm()\n        return chosen_arm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T16:44:08.690232Z","iopub.execute_input":"2024-11-06T16:44:08.691356Z","iopub.status.idle":"2024-11-06T16:44:08.706021Z","shell.execute_reply.started":"2024-11-06T16:44:08.691302Z","shell.execute_reply":"2024-11-06T16:44:08.704901Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"k = 5\nbandit = KArmBandit(k, epsilon=0.1)\n\nn_rounds = 1000 # recomendation total rounds\ntrue_rewards = [0.1, 0.5, 0.8, 0.6, 0.3] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T16:44:10.582724Z","iopub.execute_input":"2024-11-06T16:44:10.584107Z","iopub.status.idle":"2024-11-06T16:44:10.589451Z","shell.execute_reply.started":"2024-11-06T16:44:10.584047Z","shell.execute_reply":"2024-11-06T16:44:10.588214Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"for i in range(n_rounds):\n    chosen_arm = bandit.recommend()\n    reward = 1 if random.random() < true_rewards[chosen_arm] else 0\n    bandit.update(chosen_arm, reward)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T16:44:11.114161Z","iopub.execute_input":"2024-11-06T16:44:11.114568Z","iopub.status.idle":"2024-11-06T16:44:11.128556Z","shell.execute_reply.started":"2024-11-06T16:44:11.114532Z","shell.execute_reply":"2024-11-06T16:44:11.127212Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"best_arm = np.argmax(bandit.values)\nprint(f\"The best recommendation based on learned rewards is item {best_arm} with estimated reward {bandit.values[best_arm]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T16:44:11.602812Z","iopub.execute_input":"2024-11-06T16:44:11.603245Z","iopub.status.idle":"2024-11-06T16:44:11.610170Z","shell.execute_reply.started":"2024-11-06T16:44:11.603206Z","shell.execute_reply":"2024-11-06T16:44:11.608870Z"}},"outputs":[{"name":"stdout","text":"The best recommendation based on learned rewards is item 2 with estimated reward 0.8187919463087249\n","output_type":"stream"}],"execution_count":4}]}